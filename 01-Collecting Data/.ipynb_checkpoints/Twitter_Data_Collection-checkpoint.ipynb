{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0433951",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "This project is updated on 22 Oct 2022 and the aim of this project is to do a twitter analysis for one or some more keywords.\n",
    "Indeed, for our targeted keyword, we collect last 10k tweets, and then do some analysis.\n",
    "The project is done in two separted parts: \n",
    "* Collecting Data\n",
    "* Analyzing Data and Extracting Insights\n",
    "\n",
    "For doing this porject, you need to have a tweeter developer account besides having some python packages installed that will be explained during the package.\n",
    "This project is done by Darian Ghorbanian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c9167",
   "metadata": {},
   "source": [
    "### Objectives in this notebook (First Part to Collect Data):\n",
    "* Defining our Keyword/s\n",
    "* Defining our Query and Collect 10k Tweets Data\n",
    "* Collecting Authors' Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b21acc1",
   "metadata": {},
   "source": [
    "### Required Packages and other Requiremets:\n",
    "As it was mentioned above, fristly create a twitter developer account and after that, make an API which gives you keys for connecting. \n",
    "If you don't know how to do so, please follow this link: https://developer.twitter.com/en/support/twitter-api/developer-account .\n",
    "When you create the API for your software, in your developer portal at Twitter website, you'll be able to reach the data details of your API including: API Key, API Key Secret, and Bearer Token.\n",
    "\n",
    "For your data gathering, we need to have a couple of different packages installed. Here, are the list of all the packages you need for this part:\n",
    "\n",
    "* tweepy\n",
    "* json\n",
    "* pandas\n",
    "\n",
    "Most probably you know about json and pandas, but Tweepy can be something new:\n",
    "Tweepy (https://www.tweepy.org/), is an easy-to-use Python library for assessing the Twitter API. \n",
    "To install we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48232220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /opt/anaconda3/lib/python3.9/site-packages (4.10.1)\r\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from tweepy) (3.2.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /opt/anaconda3/lib/python3.9/site-packages (from tweepy) (2.27.1)\r\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from tweepy) (1.3.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236e5ae",
   "metadata": {},
   "source": [
    "## Gathering Tweets Data:\n",
    "##### The next step is to gather data using this package and the API, you've made. So, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a2f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676108e",
   "metadata": {},
   "source": [
    "Pay attention that for collecting data, we need a class made based on the tweepy.\n",
    "This class has a name TwitterCollector. You would need to have the file of related to code of this class, in the same directory that you have this notebook. Then, you would be able to run the code. As soon as you upload this file, just go for the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df0da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TwitterCollector import TwitterCollector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a27d549",
   "metadata": {},
   "source": [
    "Now, it is time to collect your data. You need your bearer_token and also an initialization for twitter_collector. \n",
    "Pay attention, 'bearer_token' should be the thing you've received from your tweeter developer account. So, pay attention to change the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7166dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token= r\"AAAAAAAAAAAAAAAAAAAAAN7xhgEAAAAA5mDxdSxnPsiFrmzQ4R2Fvet71Qs%3DeAlmpOrJCjfAhrqhwnnbuZytWstuT1qdXZFLwPE6OZTo6ka2A6\"\n",
    "#initialize it:\n",
    "tc= TwitterCollector(bearer_token=bearer_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf59354",
   "metadata": {},
   "source": [
    "#### Deciding about the query:\n",
    "Now, you are prepared to collect data. Just you need to tell about the queries you need. In this project, we are going to go for the word \"Mahsa Amini\", which is one of the greatest trends in Twitter till the end of September 2022.\n",
    "We don't want to have either \"Mahsa\" or \"Amini\", we want the exact word \"Mahsa Amini\".\n",
    "Moreover, we don't want retweets to be counted again. And we are just considering English Tweets not, Persian. Although the majority of tweets are in Persian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d7734a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1= '\"Mahsa Amini\" -is:retweet lang:en'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe16f7",
   "metadata": {},
   "source": [
    "If you want to know more about how you can define some specific queries based on your interests, read instructions here: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query\n",
    "\n",
    "For collecting tweets, we have a function fetch_recent_tweets that the inputs include query, the number of tweets we want, and saving the results or not. So, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f2be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_tweets_ma= tc.fetch_recent_tweets(query=query1, tweets_cnt=10000, save_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c939d7c7",
   "metadata": {},
   "source": [
    "When you run this code, a json file will be created in the folder that you are running the code. You need this file for the next steps to do text mining.\n",
    "\n",
    "Before going for text mining, we can have a better look on the data we collected in recent_tweets_ma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89033dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(recent_tweets_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "198761a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['collection_type', 'collection_timestamp', 'query', 'tweet_cnt', 'tweets'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recent_tweets_ma.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e115b2",
   "metadata": {},
   "source": [
    "We see that we have a dictionary with 5 different keys. The last key is 'tweets' which includes more data about a tweet. Let's have a look on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe53767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(recent_tweets_ma['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bcd7943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(recent_tweets_ma['tweets'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7069b70",
   "metadata": {},
   "source": [
    "So, we can see that 'tweet' is a list of dictionaries. Actually it includes all of our 10000 tweets and each member of this list is a dictionary which includes detail of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c6c19e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['entities', 'referenced_tweets', 'created_at', 'source', 'in_reply_to_user_id', 'lang', 'text', 'context_annotations', 'edit_history_tweet_ids', 'public_metrics', 'possibly_sensitive', 'id', 'author_id'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recent_tweets_ma['tweets'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e01398",
   "metadata": {},
   "source": [
    "The information about each tweet are the keys printed from the last line code. Again, each of these keys maybe another dictionary.\n",
    "Indeed, the json file we get from this twitter api is a dictionary of dictionary of ... of dictionaries.\n",
    "\n",
    "But, as we saw, in the data collected, we have only author_id, but not more information. So, for our holistic analyis, we need to collect them.\n",
    "\n",
    "## Gathering Authors' Data:\n",
    "The only data, we have from the tweets' data is the author_id. So, we need to collect all the unique author_ids and then, collect more data for each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2869578",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_author_id=[]\n",
    "for tweet in recent_tweets_ma['tweets']:\n",
    "    if tweet['author_id'] not in uniq_author_id:\n",
    "        uniq_author_id.append(tweet['author_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7e54877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3150\n"
     ]
    }
   ],
   "source": [
    "print(type(uniq_author_id))\n",
    "print(len(uniq_author_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cabd8b",
   "metadata": {},
   "source": [
    "As we see, we have 3150 unique author id. It means that the 10000 tweets that we've collected are made by 3150 different authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ac1dd",
   "metadata": {},
   "source": [
    "#### Automating Author Data Collection\n",
    "For collecting author's data, we need to fetch for each author. But the problem, if you don't have a professional tweeter developer accoutn, you can't fetch more than 300 times every 15 minutes. So, you need to collect your authors' data by waiting.\n",
    "So, we need to have a code as bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d7bd3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author 241 has removed their account\n",
      "we need to wait some time!!!. We have collected 281 authors data till now\n",
      "we waited 5 minutes, need to wait more 10 minutes\n",
      "we waited 10 minutes, need to wait another 5 minutes\n",
      "we will start collect data again in one minute\n",
      "Starting Again...\n",
      "we need to wait some time!!!. We have collected 429 authors data till now\n",
      "we waited 5 minutes, need to wait more 10 minutes\n",
      "we waited 10 minutes, need to wait another 5 minutes\n",
      "we will start collect data again in one minute\n",
      "Starting Again...\n",
      "we need to wait some time!!!. We have collected 729 authors data till now\n",
      "we waited 5 minutes, need to wait more 10 minutes\n",
      "we waited 10 minutes, need to wait another 5 minutes\n",
      "we will start collect data again in one minute\n",
      "Starting Again...\n",
      "we need to wait some time!!!. We have collected 1029 authors data till now\n",
      "we waited 5 minutes, need to wait more 10 minutes\n",
      "we waited 10 minutes, need to wait another 5 minutes\n",
      "we will start collect data again in one minute\n",
      "Starting Again...\n",
      "we need to wait some time!!!. We have collected 1329 authors data till now\n",
      "we waited 5 minutes, need to wait more 10 minutes\n",
      "we waited 10 minutes, need to wait another 5 minutes\n",
      "we will start collect data again in one minute\n",
      "Starting Again...\n",
      "we need to wait some time!!!. We have collected 1629 authors data till now\n",
      "we waited 5 minutes, need to wait more 10 minutes\n",
      "we waited 10 minutes, need to wait another 5 minutes\n",
      "we will start collect data again in one minute\n",
      "Starting Again...\n",
      "we need to wait some time!!!. We have collected 1929 authors data till now\n",
      "we waited 5 minutes, need to wait more 10 minutes\n",
      "we waited 10 minutes, need to wait another 5 minutes\n",
      "we will start collect data again in one minute\n",
      "Starting Again...\n",
      "we need to wait some time!!!. We have collected 2229 authors data till now\n",
      "we waited 5 minutes, need to wait more 10 minutes\n",
      "we waited 10 minutes, need to wait another 5 minutes\n",
      "we will start collect data again in one minute\n",
      "Starting Again...\n",
      "we need to wait some time!!!. We have collected 2529 authors data till now\n",
      "we waited 5 minutes, need to wait more 10 minutes\n",
      "we waited 10 minutes, need to wait another 5 minutes\n",
      "we will start collect data again in one minute\n",
      "Starting Again...\n",
      "we need to wait some time!!!. We have collected 2635 authors data till now\n",
      "we waited 5 minutes, need to wait more 10 minutes\n",
      "we waited 10 minutes, need to wait another 5 minutes\n",
      "we will start collect data again in one minute\n",
      "Starting Again...\n",
      "we need to wait some time!!!. We have collected 2935 authors data till now\n",
      "we waited 5 minutes, need to wait more 10 minutes\n",
      "we waited 10 minutes, need to wait another 5 minutes\n",
      "we will start collect data again in one minute\n",
      "Starting Again...\n"
     ]
    }
   ],
   "source": [
    "author=[]\n",
    "import time\n",
    "for i in range(0,len(uniq_author_id)):\n",
    "    try:\n",
    "        author.append(tc.fetch_author_info(uniq_author_id[i]))\n",
    "    except AttributeError: # If the an author has removed their account, we should skip it\n",
    "        print('Author ' + str(i) + ' has removed their account')\n",
    "    except:\n",
    "        print('we need to wait some time!!!. We have collected ' + str(i-1) + ' authors data till now')\n",
    "        time.sleep(5*60) #when we reach our limit of fetching, we need to wait 15 minutes\n",
    "        print('we waited 5 minutes, need to wait more 10 minutes')\n",
    "        time.sleep(5*60)\n",
    "        print('we waited 10 minutes, need to wait another 5 minutes')\n",
    "        time.sleep(4*60)\n",
    "        print('we will start collect data again in one minute')\n",
    "        time.sleep(60)\n",
    "        print('Starting Again...')\n",
    "        author.append(tc.fetch_author_info(uniq_author_id[i])) #collecting the data for author who encountered Too Many Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff483b",
   "metadata": {},
   "source": [
    "We gathered authors data in 12 rounds. As we see, an author has removed their account. Approximately 3 hours was dedicated for this task. \n",
    "It is worth mentioning that if you have a professional tweeter developer account, you should not wait as long as 3 hours for similar data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91869a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3149"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(author)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4fc80a",
   "metadata": {},
   "source": [
    "So, the number 3149 affirms that we collected data for all avialable authors. \n",
    "\n",
    "The authors data should be saved. We prefer to save it as a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4783e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('author_data.json','w') as f:\n",
    "    json.dump(author,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489528a",
   "metadata": {},
   "source": [
    "Our data is saved and the first part of our project is done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
